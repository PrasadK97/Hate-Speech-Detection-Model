{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWsWYE3Eo63W"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Text Pre-processing libraries\n",
        "import nltk\n",
        "import string\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Tensorflow imports to build the model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hate_speech.csv')\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "a6lDEkin_6-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "Rmse2c2U_7AG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "pfDU99sb_7DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(df['class'].value_counts().values,\n",
        "\t\tlabels = df['class'].value_counts().index,\n",
        "\t\tautopct='%1.1f%%')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OMO52JV1_7FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower case all the words of the tweet before any preprocessing\n",
        "df['tweet'] = df['tweet'].str.lower()\n",
        "\n",
        "# Removing punctuations present in the text\n",
        "punctuations_list = string.punctuation\n",
        "def remove_punctuations(text):\n",
        "\ttemp = str.maketrans('', '', punctuations_list)\n",
        "\treturn text.translate(temp)\n",
        "\n",
        "df['tweet']= df['tweet'].apply(lambda x: remove_punctuations(x))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "eEDPeDTc_7IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "\tstop_words = stopwords.words('english')\n",
        "\n",
        "\timp_words = []\n",
        "\n",
        "\t# Storing the important words\n",
        "\tfor word in str(text).split():\n",
        "\n",
        "\t\tif word not in stop_words:\n",
        "\n",
        "\t\t\t# Let's Lemmatize the word as well\n",
        "\t\t\t# before appending to the imp_words list.\n",
        "\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\tlemmatizer.lemmatize(word)\n",
        "\n",
        "\t\t\timp_words.append(word)\n",
        "\n",
        "\toutput = \" \".join(imp_words)\n",
        "\n",
        "\treturn output\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda text: remove_stopwords(text))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "yFZJTF55TsxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(text):\n",
        "\tstop_words = stopwords.words('english')\n",
        "\n",
        "\timp_words = []\n",
        "\n",
        "\t# Storing the important words\n",
        "\tfor word in str(text).split():\n",
        "\n",
        "\t\tif word not in stop_words:\n",
        "\n",
        "\t\t\t# Let's Lemmatize the word as well\n",
        "\t\t\t# before appending to the imp_words list.\n",
        "\n",
        "\t\t\tlemmatizer = WordNetLemmatizer()\n",
        "\t\t\tlemmatizer.lemmatize(word)\n",
        "\n",
        "\t\t\timp_words.append(word)\n",
        "\n",
        "\toutput = \" \".join(imp_words)\n",
        "\n",
        "\treturn output\n",
        "\n",
        "\n",
        "df['tweet'] = df['tweet'].apply(lambda text: remove_stopwords(text))\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "mcAEPrx1Tsyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_2 = df[df['class'] == 2]\n",
        "class_1 = df[df['class'] == 1].sample(n=3500)\n",
        "class_0 = df[df['class'] == 0]\n",
        "\n",
        "balanced_df = pd.concat([class_0, class_0, class_0, class_1, class_2], axis=0)\n"
      ],
      "metadata": {
        "id": "JwFkQKKETs2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.pie(balanced_df['class'].value_counts().values,\n",
        "\t\tlabels=balanced_df['class'].value_counts().index,\n",
        "\t\tautopct='%1.1f%%')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z9C3ALgGTtFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WOrd2Vec transformation"
      ],
      "metadata": {
        "id": "6CEAW9PKT6Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = balanced_df['tweet']\n",
        "target = balanced_df['class']\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(features,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\ttarget,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\ttest_size=0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\trandom_state=22)\n",
        "X_train.shape, X_val.shape\n"
      ],
      "metadata": {
        "id": "U0A-1hh8TtH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = pd.get_dummies(Y_train)\n",
        "Y_val = pd.get_dummies(Y_val)\n",
        "Y_train.shape, Y_val.shape\n"
      ],
      "metadata": {
        "id": "kPLlXK8TTtK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 5000\n",
        "max_len = 100\n",
        "\n",
        "token = Tokenizer(num_words=max_words,\n",
        "\t\t\t\tlower=True,\n",
        "\t\t\t\tsplit=' ')\n",
        "\n",
        "token.fit_on_texts(X_train)\n"
      ],
      "metadata": {
        "id": "pvCsnuhOUBj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training the tokenizer\n",
        "max_words = 5000\n",
        "token = Tokenizer(num_words=max_words,\n",
        "\t\t\t\tlower=True,\n",
        "\t\t\t\tsplit=' ')\n",
        "token.fit_on_texts(train_X)\n",
        "\n",
        "#Generating token embeddings\n",
        "Training_seq = token.texts_to_sequences(train_X)\n",
        "Training_pad = pad_sequences(Training_seq,\n",
        "\t\t\t\t\t\t\tmaxlen=50,\n",
        "\t\t\t\t\t\t\tpadding='post',\n",
        "\t\t\t\t\t\t\ttruncating='post')\n",
        "\n",
        "Testing_seq = token.texts_to_sequences(test_X)\n",
        "Testing_pad = pad_sequences(Testing_seq,\n",
        "\t\t\t\t\t\t\tmaxlen=50,\n",
        "\t\t\t\t\t\t\tpadding='post',\n",
        "\t\t\t\t\t\t\ttruncating='post')\n"
      ],
      "metadata": {
        "id": "0GpvNSR4UBlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "\tlayers.Embedding(max_words, 32, input_length=max_len),\n",
        "\tlayers.Bidirectional(layers.LSTM(16)),\n",
        "\tlayers.Dense(512, activation='relu', kernel_regularizer='l1'),\n",
        "\tlayers.BatchNormalization(),\n",
        "\tlayers.Dropout(0.3),\n",
        "\tlayers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "\t\t\toptimizer='adam',\n",
        "\t\t\tmetrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "QuG24qB_UBnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(\n",
        "\tmodel,\n",
        "\tshow_shapes=True,\n",
        "\tshow_dtype=True,\n",
        "\tshow_layer_activations=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "ILt87JSuUBo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(\n",
        "\tmodel,\n",
        "\tshow_shapes=True,\n",
        "\tshow_dtype=True,\n",
        "\tshow_layer_activations=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "UWFoW6AGUBsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, Y_train,\n",
        "\t\t\t\t\tvalidation_data=(X_val, Y_val),\n",
        "\t\t\t\t\tepochs=50,\n",
        "\t\t\t\t\tverbose=1,\n",
        "\t\t\t\t\tbatch_size=32,\n",
        "\t\t\t\t\tcallbacks=[lr, es])\n"
      ],
      "metadata": {
        "id": "oV_xwsEHUPT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "history_df.loc[:, ['loss', 'val_loss']].plot()\n",
        "history_df.loc[:, ['accuracy', 'val_accuracy']].plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "D6ZQEdQ3UPVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ty7GzcdvUPX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQQv9MhZUPa5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}