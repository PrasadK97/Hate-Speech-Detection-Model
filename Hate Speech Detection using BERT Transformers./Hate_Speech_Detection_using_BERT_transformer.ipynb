{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai3R52DotviU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "import random\n",
        "import operator\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "\n",
        "# !pip install transformers\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(path, sample_size=5, cols=['text', 'HOF'], label=None):\n",
        "    \"\"\"Helper function that loads data from a given path into a pandas\n",
        "       DataFrame, using only the specified cols. Also prints basic info\n",
        "       about the dataset size and displays a sample of the rows.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(path, sep='\\t', usecols=cols)\n",
        "\n",
        "    print(f\"\\nThere are {df.shape[0]} tweets in the {label} dataset.\")\n",
        "    print(\"\\nHere's a sample:\\n\")\n",
        "    display(df.sample(sample_size))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "_cTuFROTVFCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab\n",
        "train = load_data('train.tsv', label='train')\n",
        "test = load_data('test.tsv', label='test')"
      ],
      "metadata": {
        "id": "olp0ySSAVFEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QbC2AFePVFGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "1XriOl2VVQuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map labels to binary integers\n",
        "label2id = {'Non-Hateful': 0, 'Hateful': 1}\n",
        "train['HOF'] = train['HOF'].apply(lambda x: label2id[x])\n",
        "test['HOF'] = test['HOF'].apply(lambda x: label2id[x])"
      ],
      "metadata": {
        "id": "2y0O0iqtVFIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before downsampling: ')\n",
        "print(f\"Hateful: {len(train[train['HOF']==1])}\")\n",
        "print(f\"Non-Hateful: {len(train[train['HOF']==0])}\")\n",
        "\n",
        "train_hateful = train[train['HOF']==1]\n",
        "train_nonhateful = train[train['HOF']==0].sample(len(train_hateful))\n",
        "train_downsampled = pd.concat([train_hateful, train_nonhateful], axis=0).sample(frac=1)\n",
        "\n",
        "print('\\nAfter downsampling: ')\n",
        "print(f\"Hateful: {len(train_downsampled[train_downsampled['HOF']==1])}\")\n",
        "print(f\"Non-Hateful: {len(train_downsampled[train_downsampled['HOF']==0])}\")"
      ],
      "metadata": {
        "id": "UEKgx7hXVFKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data splitting"
      ],
      "metadata": {
        "id": "CXTk0J1KVbJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Split train data set into train and development sets\n",
        "train, dev = train_test_split(train_downsampled, test_size=0.5, stratify=train_downsampled['HOF'])"
      ],
      "metadata": {
        "id": "HbCfnmwbVdY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the tweets' text\n",
        "def clean_text(tweet):\n",
        "    \"\"\"A function that performs basic cleaning of a tweet's text.\n",
        "    \"\"\"\n",
        "\n",
        "    # Replace mentions and URLs with special token\n",
        "    tweet = re.sub(r\"@[A-Za-z0-9_-]+\",'USR',tweet)\n",
        "    tweet = re.sub(r\"http\\S+\",'URL',tweet)\n",
        "\n",
        "    # Remove \\n and \\t characters\n",
        "    tweet = tweet.replace('\\n', ' ')\n",
        "    tweet = tweet.replace('[NEWLINE]', ' ')\n",
        "    tweet = tweet.replace('\\t', ' ')\n",
        "\n",
        "    # Strip whitespace\n",
        "    tweet = tweet.strip()\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tweet = tweet.lower()\n",
        "\n",
        "    # return [w.strip(punctuation) for w in tweet.split() if w.strip(punctuation)!='']\n",
        "    return tweet\n",
        "\n",
        "# train['cleaned_text'] = train['text'].apply(lambda x: clean_text(x))\n",
        "# test['cleaned_text'] = test['text'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "yD3YNHITVk4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data encoding and tokenizing"
      ],
      "metadata": {
        "id": "so5lOH2lVmNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Dataset class which cleans, tokenizes and encodes data\n",
        "class BERTDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "\n",
        "        # Initialize BERT tokenizer\n",
        "        # Note that I need to specify cache_dir because I'm using a venv\n",
        "        self.tok = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=Path.cwd()/'venv/lib/python3.8/site-packages')\n",
        "\n",
        "        # Clean tweets\n",
        "        self.cleaned_tweets = data['text'].apply(lambda x: clean_text(x))\n",
        "\n",
        "        # Truncate and encode tweets, up to max_length of 60\n",
        "        # While this is lower than BERT's max (512), it was chosen for computational speed\n",
        "        self.tweets = list(self.cleaned_tweets.apply(self.tok.encode, max_length=60, truncation=True))\n",
        "\n",
        "        # Store labels\n",
        "        self.labels = list(data['HOF'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tweet = self.tweets[idx]\n",
        "        label = self.labels[idx]\n",
        "        return tweet, label\n",
        "\n",
        "# Inspect an example\n",
        "# BD = BERTDataset(train.iloc[:5])\n",
        "# next(iter(BD))"
      ],
      "metadata": {
        "id": "CUNkrviPVk6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define collate function to be passed to DataLoader\n",
        "def bert_collate(batch):\n",
        "\n",
        "    # Store batch size\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    # Separate tweets and labels\n",
        "    tweets = [t for t, _ in batch]\n",
        "    labels = torch.tensor([l for _, l in batch]).long()\n",
        "\n",
        "    # Store length of longest tweet in batch\n",
        "    max_len = max(len(t) for t in tweets)\n",
        "\n",
        "    # Create padded tweet and attention mask tensors\n",
        "    tweets_pad = torch.zeros((batch_size, max_len)).long()\n",
        "    masks_pad = torch.zeros((batch_size, max_len)).long()\n",
        "    for i, t in enumerate(tweets):\n",
        "        tweets_pad[i, :len(t)] = torch.tensor(t)\n",
        "        masks_pad[i, :len(t)] = 1\n",
        "\n",
        "    return tweets_pad, masks_pad, labels"
      ],
      "metadata": {
        "id": "X1kX3gqnVk87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Create data sets\n",
        "train_dataset = BERTDataset(train)\n",
        "dev_dataset = BERTDataset(dev)\n",
        "test_dataset = BERTDataset(test)"
      ],
      "metadata": {
        "id": "V6jrfTTbVk-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders using torch.utils.data.DataLoader class\n",
        "# Using shuffle=True instead of specifying RandomSampler\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, collate_fn=bert_collate, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=100, collate_fn=bert_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, collate_fn=bert_collate)"
      ],
      "metadata": {
        "id": "WpwZbK9gVyA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect\n",
        "for (idx, batch) in enumerate(train_loader):\n",
        "\n",
        "    print(f'\\n\\n--------------------- Batch {idx} ---------------------\\n')\n",
        "\n",
        "    # Print the text\n",
        "    print(f\"There are {len(batch[0])} encoded tweets in this batch.\")\n",
        "    print('Tweets (encoded): ', batch[0])\n",
        "\n",
        "    # Print the label\n",
        "    print(f\"There are {len(batch[2])} encoded labels in this batch. Here they are: \")\n",
        "    print('Labels: ', batch[2])"
      ],
      "metadata": {
        "id": "1dL7FuQ-VyDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BERT classifier\n",
        "class BERTClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # Specify network layers\n",
        "        # Note that I need to specify cache as I'm using a venv\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased', cache_dir=Path.cwd()/'venv/lib/python3.8/site-packages')\n",
        "        self.linear = nn.Linear(768, 4)\n",
        "\n",
        "        # Define dropout\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # Freeze BERT layers\n",
        "        for n, p in self.bert.named_parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, tweets, masks):\n",
        "\n",
        "        # Define flow of tensors through the network\n",
        "        output_bert = self.bert(tweets, attention_mask=masks)[0].mean(axis=1)\n",
        "        return self.linear(self.dropout(output_bert))"
      ],
      "metadata": {
        "id": "YS3e4K5KVyFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise model\n",
        "model = BERTClassifier()"
      ],
      "metadata": {
        "id": "xJwAQDFdVlBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "TlRG3YJyV77k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimiser, objective function and epochs\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "7EopAeHjV7-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Train model\n",
        "for epoch_i in range(1, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    # Put model into training mode. This is necessary so that the `Dropout`\n",
        "    # layers are activated.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of the training data...\n",
        "    for i, batch in enumerate(tqdm(train_loader)):\n",
        "\n",
        "        # Step 1. Since PyTorch accumulates gradients, clear any previously\n",
        "        # calculated gradients before performing a backward pass.\n",
        "        # PyTorch doesn't do this automatically because it can be useful while\n",
        "        # training RNNs.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Step 2. Extract data and move to device.\n",
        "        tweets, masks, labels = [t.to(device) for t in batch]\n",
        "\n",
        "        # Step 3. Forward pass - note that calling `model()` will in turn call\n",
        "        # the model's `forward()` function.\n",
        "        output = model(tweets, masks)\n",
        "\n",
        "        # Step 4. Compute loss.\n",
        "        loss = criterion(output, labels)\n",
        "\n",
        "        # Step 5. Perform backward pass to calculate gradients wrt each w and b term.\n",
        "        loss.backward()\n",
        "\n",
        "        # Step 6. Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Step 7. Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    # Put model into evaluation mode, thereby deactivating Dropout layer.\n",
        "    model.eval()\n",
        "\n",
        "    y_true = list()\n",
        "    y_pred = list()\n",
        "\n",
        "    with torch.no_grad(): # We no longer need it to store computation graph.\n",
        "        for batch in dev_loader:\n",
        "            tweets, masks, labels = [t.to(device) for t in batch]\n",
        "            output = model(tweets, masks)\n",
        "            max_output = output.argmax(dim=1)\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(max_output.tolist())\n",
        "\n",
        "    print(f\"Accuracy after {epoch_i} epoch(s): {accuracy_score(y_true, y_pred)}\")"
      ],
      "metadata": {
        "id": "oMWIVkCkV8AI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "\n",
        "# ========================================\n",
        "#               Evaluation\n",
        "# ========================================\n",
        "\n",
        "# Evaluate model on test data\n",
        "model.eval()\n",
        "\n",
        "y_true = list()\n",
        "y_pred = list()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        tweets, masks, labels = [t.to(device) for t in batch]\n",
        "        output = model(tweets, masks)\n",
        "        max_output = output.argmax(dim=1)\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(max_output.tolist())\n",
        "\n",
        "print('Test accuracy: {:.2f}'.format(accuracy_score(y_true, y_pred)))\n",
        "print('\\nClassification report: \\n', classification_report(y_true, y_pred))\n",
        "print('\\nConfusion matrix: \\n')\n",
        "display(pd.DataFrame({\"Predicted: Unhateful\": confusion_matrix(y_true, y_pred)[:, 0],\n",
        "              \"Predicted: Hateful\": confusion_matrix(y_true, y_pred)[:, 1]},\n",
        "             index=['Actual: Unhateful', 'Actual: Hateful']))"
      ],
      "metadata": {
        "id": "fKKQyw6aV8CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vu3X8mS4V8EO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vktlnjkLV8Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pC14TzzyVlCd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}